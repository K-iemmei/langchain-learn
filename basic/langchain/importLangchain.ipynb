{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6286a013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70e841",
   "metadata": {},
   "source": [
    "##### 1. Agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ec5782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "855aeb67",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      5\u001b[39m graph = create_agent(\n\u001b[32m      6\u001b[39m     model = \u001b[33m\"\u001b[39m\u001b[33mgpt-3.5-turbo\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     tools=[check_weather],\n\u001b[32m      8\u001b[39m     system_prompt=\u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m inputs  = {\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     12\u001b[39m     [{\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     }]\n\u001b[32m     16\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langgraph\\pregel\\main.py:2679\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2677\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2678\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2684\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2685\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2688\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2689\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langchain\\agents\\factory.py:1034\u001b[39m, in \u001b[36mcreate_agent.<locals>.model_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1021\u001b[39m request = ModelRequest(\n\u001b[32m   1022\u001b[39m     model=model,\n\u001b[32m   1023\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1029\u001b[39m     runtime=runtime,\n\u001b[32m   1030\u001b[39m )\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;66;03m# No handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     response = \u001b[43m_execute_model_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1036\u001b[39m     \u001b[38;5;66;03m# Call composed handler with base handler\u001b[39;00m\n\u001b[32m   1037\u001b[39m     response = wrap_model_call_handler(request, _execute_model_sync)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langchain\\agents\\factory.py:1007\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_sync\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_prompt:\n\u001b[32m   1005\u001b[39m     messages = [SystemMessage(request.system_prompt), *messages]\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m output = \u001b[43mmodel_\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# Handle model output to get messages and structured_response\u001b[39;00m\n\u001b[32m   1010\u001b[39m handled_output = _handle_model_output(output, effective_response_format)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5492\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5485\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5486\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5487\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5490\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5491\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5494\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5495\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5496\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:379\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    367\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    372\u001b[39m     **kwargs: Any,\n\u001b[32m    373\u001b[39m ) -> AIMessage:\n\u001b[32m    374\u001b[39m     config = ensure_config(config)\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    376\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    377\u001b[39m         cast(\n\u001b[32m    378\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    389\u001b[39m         ).message,\n\u001b[32m    390\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1088\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1079\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1080\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1081\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m     **kwargs: Any,\n\u001b[32m   1086\u001b[39m ) -> LLMResult:\n\u001b[32m   1087\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:903\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    902\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    909\u001b[39m         )\n\u001b[32m    910\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    911\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1192\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1190\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1191\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1196\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1299\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1298\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1299\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1301\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1302\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1303\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1304\u001b[39m ):\n\u001b[32m   1305\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1294\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1287\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1288\u001b[39m             response,\n\u001b[32m   1289\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1290\u001b[39m             metadata=generation_info,\n\u001b[32m   1291\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1292\u001b[39m         )\n\u001b[32m   1293\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1294\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1295\u001b[39m         response = raw_response.parse()\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1156\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1110\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1112\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1153\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1154\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1155\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "During task with name 'model' and id 'df30947e-ad55-ea0b-06cb-1fe20e9e98dd'"
     ]
    }
   ],
   "source": [
    "def check_weather(location: str) ->str: \n",
    "    '''Return weather forcast for the special location'''\n",
    "    return f\"It's always sunny in {location}\" \n",
    "\n",
    "graph = create_agent(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    tools=[check_weather],\n",
    "    system_prompt=\"You are a helpful assistant\"\n",
    ")\n",
    "inputs  = {\n",
    "    \"messages\":\n",
    "    [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\" : \"What is the weather in NYC ?\"\n",
    "    }]\n",
    "}\n",
    "for chunk in graph.stream(inputs, stream_mode = \"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b70fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Answer]: content='The capital of the USA is **Washington, D.C.**' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'} id='lc_run--ec2f1adc-e322-4bc9-becf-e7c154679fcd-0' usage_metadata={'input_tokens': 8, 'output_tokens': 42, 'total_tokens': 50, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 29}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI \n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",  \n",
    "    temperature=0.1,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa13e0e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fc1edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic tool \n",
    "def check_weather(location: str) ->str: \n",
    "    '''Return weather forcast for the special location'''\n",
    "    return f\"It's always sunny in {location}\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23234109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'check_weather', 'arguments': '{\"location\": \"NYC\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--87d08212-4861-4c51-85a4-74d3e78e2dc7-0', tool_calls=[{'name': 'check_weather', 'args': {'location': 'NYC'}, 'id': '17f10bfa-c1ee-4af3-a507-dcbda949ec08', 'type': 'tool_call'}], usage_metadata={'input_tokens': 53, 'output_tokens': 64, 'total_tokens': 117, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 49}})]}}\n",
      "{'tools': {'messages': [ToolMessage(content=\"It's always sunny in NYC\", name='check_weather', id='7087bc6f-4602-466c-a7a8-a1faa8108a6e', tool_call_id='17f10bfa-c1ee-4af3-a507-dcbda949ec08')]}}\n",
      "{'model': {'messages': [AIMessage(content=\"It's always sunny in NYC\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--55527117-06b2-4ccb-b67e-54116cd3ae69-0', usage_metadata={'input_tokens': 89, 'output_tokens': 7, 'total_tokens': 96, 'input_token_details': {'cache_read': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "## basic agent \n",
    "graph = create_agent(\n",
    "    model = llm,\n",
    "    tools=[check_weather],\n",
    "    system_prompt=\"You are a helpful assistant\"\n",
    ")\n",
    "inputs  = {\n",
    "    \"messages\":\n",
    "    [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\" : \"What is the weather in NYC ?\"\n",
    "    }]\n",
    "}\n",
    "for chunk in graph.stream(inputs, stream_mode = \"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "390beed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## custome agent \n",
    "## middleware \n",
    "from langchain.agents.middleware import SummarizationMiddleware, HumanInTheLoopMiddleware\n",
    "\n",
    "summ_middleware = SummarizationMiddleware(\n",
    "    model=llm,\n",
    "    messages_to_keep=20,\n",
    "    summary_prompt=\"Custom prompt for summarization...\"\n",
    ")\n",
    "graph = create_agent(\n",
    "    model = llm,\n",
    "    tools=[check_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    "        middleware=[summ_middleware],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13fec037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SummarizationMiddleware.before_model': None}\n",
      "{'model': {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'check_weather', 'arguments': '{\"location\": \"NYC\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--179058cd-7fac-45a7-8498-0e9aceb4f45c-0', tool_calls=[{'name': 'check_weather', 'args': {'location': 'NYC'}, 'id': '47b696cd-261b-4313-a122-a1e89b90e652', 'type': 'tool_call'}], usage_metadata={'input_tokens': 75, 'output_tokens': 64, 'total_tokens': 139, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 49}})]}}\n",
      "{'tools': {'messages': [ToolMessage(content=\"It's always sunny in NYC\", name='check_weather', id='17897587-4dd4-4a89-a6ad-ad21528aaafe', tool_call_id='47b696cd-261b-4313-a122-a1e89b90e652')]}}\n",
      "{'SummarizationMiddleware.before_model': None}\n",
      "{'model': {'messages': [AIMessage(content=\"It's always sunny in NYC.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--cbf3a90d-9318-4fc9-a98c-80d4334ba8e3-0', usage_metadata={'input_tokens': 111, 'output_tokens': 8, 'total_tokens': 119, 'input_token_details': {'cache_read': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "inputs  = {\n",
    "    \"messages\":\n",
    "    [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\" : \"Please, Let me know the detail of question that What is the weather in NYC, i really want to know because i want to hangout tomorrow?\"\n",
    "    }]\n",
    "}\n",
    "for chunk in graph.stream(inputs, stream_mode = \"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30999ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary ===\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SummarizationMiddleware' object has no attribute '_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(summ_middleware.summary)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Một số phiên bản lưu summary trong metadata\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43msumm_middleware\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_state\u001b[49m.metadata.get(\u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mAttributeError\u001b[39m: 'SummarizationMiddleware' object has no attribute '_state'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45c60999",
   "metadata": {},
   "source": [
    "##### 2. Middleware "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d8de91",
   "metadata": {},
   "source": [
    "##### 3. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa54c0a",
   "metadata": {},
   "source": [
    "Có hai cách tạo chat model trong langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e7dbda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_chat_model(\n",
    "#     model: str | None = None,\n",
    "#     *,\n",
    "#     model_provider: str | None = None,\n",
    "#     configurable_fields: Literal[\"any\"] | list[str] | tuple[str, ...] | None = None,\n",
    "#     config_prefix: str | None = None,\n",
    "#     **kwargs: Any,\n",
    "# ) -> BaseChatModel | _ConfigurableModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "81bf4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637bfe4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I am a large language model, trained by Google.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--ba9c5a6f-5a3e-4a39-a954-cfc51e9e7b1a-0', usage_metadata={'input_tokens': 6, 'output_tokens': 36, 'total_tokens': 42, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 25}})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = init_chat_model(model = \"gemini-2.5-flash\", model_provider=\"google_genai\")  #BaseChatModel \n",
    "model1.invoke(\"what's your name\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a9bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I do not have a name. I am a large language model, trained by Google.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--0838259c-a0e8-4924-903a-4fe920078025-0', usage_metadata={'input_tokens': 7, 'output_tokens': 53, 'total_tokens': 60, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 35}})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = ChatGoogleGenerativeAI(model = \"gemini-2.5-flash\")  #BaseChatModel \n",
    "model2.invoke(\"what's your name ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e69cafe",
   "metadata": {},
   "source": [
    "Một vài optional parameter \n",
    "\n",
    "    * temperature: Model temperature for controlling randomness.\n",
    "    * max_tokens: Maximum number of output tokens.\n",
    "    * timeout: Maximum time (in seconds) to wait for a response.\n",
    "    * max_retries: Maximum number of retry attempts for failed requests.\n",
    "    * base_url: Custom API endpoint URL.\n",
    "    * rate_limiter: A BaseRateLimiter instance to control request rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eef96ff",
   "metadata": {},
   "source": [
    "##### 4. Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca8da41",
   "metadata": {},
   "source": [
    "AIMessage : return from a chat model as a response to a prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import AIMessage \n",
    "\"\"\"\n",
    "Attr \n",
    "\n",
    "    - tool_calls: list tool call \n",
    "    - invalid_tool_call: list tool call parsing error \n",
    "    - usage_metadata: \n",
    "    - content_block: contetn \n",
    "\"\"\"\n",
    "aimessage_example = model1.invoke(\"Who is Trump ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "81f78bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Calls:  []\n",
      "Invalid Tool Call  []\n",
      "Usage Metadata  {'input_tokens': 5, 'output_tokens': 1776, 'total_tokens': 1781, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 979}}\n",
      "Content Blocks  [{'type': 'text', 'text': 'Donald John Trump is an American businessman, media personality, and politician who served as the **45th President of the United States** from 2017 to 2021.\\n\\nHere\\'s a breakdown of his background and career:\\n\\n1.  **Early Life and Business Career:**\\n    *   Born in Queens, New York, in 1946.\\n    *   He joined his father\\'s real estate business, Elizabeth Trump & Son, in 1968, which he later renamed The Trump Organization.\\n    *   He became a prominent real estate developer, building and acquiring numerous hotels, casinos, golf courses, and other properties worldwide. His name became a globally recognized brand.\\n\\n2.  **Media Personality:**\\n    *   He gained significant public recognition as a television personality, most notably as the host of the reality TV show \"The Apprentice\" from 2004 to 2015, where he famously used the catchphrase \"You\\'re fired!\"\\n\\n3.  **Political Career (Pre-Presidency):**\\n    *   Trump had flirted with presidential runs in previous cycles (1988, 2000, 2012) but never fully committed until 2015.\\n    *   He was a prominent figure in the \"birther\" movement, questioning the legitimacy of Barack Obama\\'s presidency.\\n\\n4.  **2016 Presidential Campaign:**\\n    *   Trump announced his candidacy for president as a Republican in June 2015.\\n    *   His campaign focused on themes like \"Make America Great Again,\" stricter immigration policies (including building a wall on the U.S.-Mexico border), renegotiating trade deals, and an \"America First\" foreign policy.\\n    *   Despite never having held elected office, he won the 2016 presidential election, defeating Democratic candidate Hillary Clinton, in a victory that surprised many political pundits.\\n\\n5.  **Presidency (2017-2021):**\\n    *   **Domestic Policy:** Signed the Tax Cuts and Jobs Act of 2017, appointed three conservative justices to the Supreme Court (Neil Gorsuch, Brett Kavanaugh, Amy Coney Barrett), pursued deregulation, and attempted to repeal and replace the Affordable Care Act (Obamacare).\\n    *   **Foreign Policy:** Withdrew the U.S. from the Trans-Pacific Partnership trade agreement, the Paris Agreement on climate change, and the Iran nuclear deal. Moved the U.S. embassy in Israel to Jerusalem.\\n    *   **Controversies:** His presidency was marked by numerous controversies, including investigations into Russian interference in the 2016 election, two impeachment proceedings by the House of Representatives (though he was acquitted by the Senate both times), and his administration\\'s response to the COVID-19 pandemic.\\n    *   **Communication Style:** Known for his unconventional communication style, frequent use of social media (especially Twitter), and direct appeals to his base.\\n\\n6.  **Post-Presidency:**\\n    *   He lost his bid for re-election in 2020 to Democrat Joe Biden.\\n    *   His efforts to challenge the election results, culminating in the January 6, 2021, attack on the U.S. Capitol, were highly controversial.\\n    *   Since leaving office, Trump has remained a dominant figure in the Republican Party, endorsing candidates, holding rallies, and maintaining a strong base of support.\\n    *   He is currently a leading candidate for the Republican nomination in the 2024 presidential election.\\n\\nIn summary, Donald Trump is a highly influential and often polarizing figure in American politics and culture, with a significant impact on both domestic and international affairs.'}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tool Calls: \", aimessage_example.tool_calls)\n",
    "print(\"Invalid Tool Call \", aimessage_example.invalid_tool_calls)\n",
    "print(\"Usage Metadata \", aimessage_example.usage_metadata)\n",
    "print(\"Content Blocks \", aimessage_example.content_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de702d63",
   "metadata": {},
   "source": [
    "AIMessageChunk: Message chunk from an AI(yielded when streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import AIMessageChunk \n",
    "\"\"\"\n",
    "Attr \n",
    "\n",
    "    type\tThe type of the message (used for deserialization).\n",
    "        TYPE: Literal['AIMessageChunk']\n",
    "\n",
    "    tool_call_chunks\tIf provided, tool call chunks associated with the message.\n",
    "        TYPE: list[ToolCallChunk]\n",
    "\n",
    "    chunk_position\tOptional span represented by an aggregated AIMessageChunk.\n",
    "        TYPE: Literal['last'] | None\n",
    "\n",
    "    lc_attributes\tAttributes to be serialized, even if they are derived from other initialization args.\n",
    "        TYPE: dict\n",
    "\n",
    "    content_blocks\tReturn standard, typed ContentBlock dicts from the message.\n",
    "        TYPE: list[ContentBlock]\n",
    "\"\"\"\n",
    "\n",
    "aiMessageChunk_example = model1.stream(\"Who is Trump ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "466c75bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_message_chunk_list = []\n",
    "for chunk in aiMessageChunk_example:\n",
    "    if isinstance(chunk, AIMessageChunk):\n",
    "        AI_message_chunk_list.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b1cc1525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessageChunk(content=\"Donald John Trump is an American businessman, media personality, and politician who served as the **45th President of the United States from 2017 to 2021**.\\n\\nHere's a breakdown of his background\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 5, 'output_tokens': 1128, 'total_tokens': 1133, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1080}}),\n",
       " AIMessageChunk(content=\" and career:\\n\\n1.  **Early Life and Business Career:**\\n    *   Born in Queens, New York, in 1946.\\n    *   He inherited control of his father Fred Trump's real estate and\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 0, 'total_tokens': 49, 'output_token_details': {'reasoning': 0}, 'input_token_details': {'cache_read': 0}, 'output_tokens': 49}),\n",
       " AIMessageChunk(content=' construction firm, which he later renamed **The Trump Organization**.\\n    *   He expanded its operations significantly, developing and owning hotels, casinos, golf courses, and various other ventures, primarily in New York City and other major markets.\\n\\n', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 0, 'total_tokens': 47, 'output_token_details': {'reasoning': 0}, 'input_token_details': {'cache_read': 0}, 'output_tokens': 47}),\n",
       " AIMessageChunk(content='2.  **Media Personality:**\\n    *   Trump gained significant public recognition as a television personality, most notably as the host of the reality TV show **\"The Apprentice\"** for many years, starting in 2004.', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 0, 'total_tokens': 49, 'output_token_details': {'reasoning': 0}, 'input_token_details': {'cache_read': 0}, 'output_tokens': 49}),\n",
       " AIMessageChunk(content='\\n    *   He also authored several books, including \"The Art of the Deal.\"\\n\\n3.  **Political Career (Pre-Presidency):**\\n    *   He had flirted with presidential runs in previous cycles (198', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 0, 'total_tokens': 48, 'output_token_details': {'reasoning': 0}, 'input_token_details': {'cache_read': 0}, 'output_tokens': 48}),\n",
       " AIMessageChunk(content='8, 2000, 2012) and was a prominent media commentator on political issues.\\n\\n4.  **Presidency (2017-2021):**\\n    *   He', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 0, 'total_tokens': 48, 'output_token_details': {'reasoning': 0}, 'input_token_details': {'cache_read': 0}, 'output_tokens': 48}),\n",
       " AIMessageChunk(content=' ran for president as a Republican in 2016, campaigning on a populist platform with the slogan \"Make America Great Again,\" and surprisingly defeated Democratic nominee Hillary Clinton.\\n    *   **Key policies and events during his presidency included:**\\n        ', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 0, 'total_tokens': 49, 'output_token_details': {'reasoning': 0}, 'input_token_details': {'cache_read': 0}, 'output_tokens': 49}),\n",
       " AIMessageChunk(content='*   **Economic policies:** Tax cuts (Tax Cuts and Jobs Act of 2017) and deregulation.\\n        *   **Immigration:** A focus on stricter immigration enforcement, including efforts to build a wall on the U.', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 0, 'total_tokens': 49, 'output_token_details': {'reasoning': 0}, 'input_token_details': {'cache_read': 0}, 'output_tokens': 49}),\n",
       " AIMessageChunk(content='S.-Mexico border and travel bans from several Muslim-majority countries.\\n        *   **Trade:** A protectionist trade agenda, renegotiating NAFTA (creating the USMCA), and imposing tariffs on goods from China and other countries.\\n        ', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 0, 'total_tokens': 48, 'output_token_details': {'reasoning': 0}, 'input_token_details': {'cache_read': 0}, 'output_tokens': 48}),\n",
       " AIMessageChunk(content='*   **Foreign Policy:** An \"America First\" approach, withdrawing the U.S. from the Paris Agreement on climate change and the Iran nuclear deal, and moving the U.S. embassy in Israel to Jerusalem.\\n        *   ', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 0, 'total_tokens': 48, 'output_token_details': {'reasoning': 0}, 'input_token_details': {'cache_read': 0}, 'output_tokens': 48}),\n",
       " AIMessageChunk(content='**Judiciary:** Appointed three conservative justices to the Supreme Court (Neil Gorsuch, Brett Kavanaugh, and Amy Coney Barrett).\\n        *   **Impeachments:** He was impeached twice by the House of Representatives, first in ', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 0, 'total_tokens': 49, 'output_token_details': {'reasoning': 0}, 'input_token_details': {'cache_read': 0}, 'output_tokens': 49}),\n",
       " AIMessageChunk(content='2019 over his dealings with Ukraine, and again in 2021 following the January 6th Capitol riot. He was acquitted by the Senate in both instances.\\n\\n5.  **Post-Presidency:**\\n', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 0, 'total_tokens': 47, 'output_token_details': {'reasoning': 0}, 'input_token_details': {'cache_read': 0}, 'output_tokens': 47}),\n",
       " AIMessageChunk(content='    *   Since leaving office in 2021, Trump has remained a dominant and highly influential figure in Republican politics.\\n    *   He is currently the **presumptive Republican nominee for the 2024 presidential election**, seeking', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 0, 'total_tokens': 50, 'output_token_details': {'reasoning': 0}, 'input_token_details': {'cache_read': 0}, 'output_tokens': 50}),\n",
       " AIMessageChunk(content=' to return to the White House.\\n    *   He has also faced numerous **legal challenges**, including civil lawsuits and criminal indictments related to his business practices, his efforts to overturn the 2020 election results, and his', additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 0, 'total_tokens': 48, 'output_token_details': {'reasoning': 0}, 'input_token_details': {'cache_read': 0}, 'output_tokens': 48}),\n",
       " AIMessageChunk(content=\" handling of classified documents after leaving office.\\n\\nTrump's political style is often characterized by his populist rhetoric, extensive use of social media, and an unconventional approach to traditional politics. He remains a highly influential and polarizing figure in American politics and society.\", additional_kwargs={}, response_metadata={'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 0, 'total_tokens': 50, 'output_token_details': {'reasoning': 0}, 'input_token_details': {'cache_read': 0}, 'output_tokens': 50}),\n",
       " AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', usage_metadata={'input_tokens': 0, 'total_tokens': 0, 'output_token_details': {'reasoning': 0}, 'input_token_details': {'cache_read': 0}, 'output_tokens': 0}),\n",
       " AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--c12c4f56-e35e-4e40-8403-fc2a5e79a2ae', chunk_position='last')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_message_chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "68fad92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessageChunk\n",
      "[]\n",
      "None\n",
      "[{'type': 'text', 'text': \" and career:\\n\\n1.  **Early Life and Business Career:**\\n    *   Born in Queens, New York, in 1946.\\n    *   He inherited control of his father Fred Trump's real estate and\"}]\n"
     ]
    }
   ],
   "source": [
    "chunk1 = AI_message_chunk_list[1]\n",
    "print(chunk1.type)\n",
    "print(chunk1.tool_call_chunks)\n",
    "print(chunk1.chunk_position)\n",
    "print(chunk1.content_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4182b",
   "metadata": {},
   "source": [
    "HumanMessage : is a message that is passed in from a user to the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d3964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage \n",
    "\"\"\"\n",
    "Attr \n",
    "\n",
    "    type: Literal['human'] = 'human'\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a369ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = HumanMessage(content=\"What is your name?\")\n",
    "response = model1.invoke([message]) # -> AI Message "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c04a3154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n"
     ]
    }
   ],
   "source": [
    "print(message.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0113e",
   "metadata": {},
   "source": [
    "SystemMessagee :  is usually passed in as the first of a sequence of input messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e343578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import SystemMessage \n",
    "\"\"\"\n",
    "Attr \n",
    "\n",
    "    type: Literal['system'] = 'system'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d5070bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n"
     ]
    }
   ],
   "source": [
    "sMessage = SystemMessage(\"You are a helpful assistant!!!\") \n",
    "print(sMessage.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0258c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='My name is Bob.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'} id='lc_run--b42cbd1f-c569-4961-a02f-c9d7486ac8be-0' usage_metadata={'input_tokens': 17, 'output_tokens': 32, 'total_tokens': 49, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 27}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant! Your name is Bob.\"),\n",
    "    HumanMessage(content=\"What is your name?\"),\n",
    "]\n",
    "\n",
    "print(model1.invoke(messages)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76241f67",
   "metadata": {},
   "source": [
    "ToolMessage : objects contain the result of a tool invocation. Typically, the result is encoded inside the content field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7a73ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import ToolMessage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8f51ec4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='From the graph we can see that the correlation between x and y is ...', tool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL', artifact={'stdout': 'From the graph we can see that the correlation between x and y is ...', 'stderr': None, 'artifacts': {'type': 'image', 'base64_data': '/9j/4gIcSU...'}})"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "tool_output = {\n",
    "    \"stdout\": \"From the graph we can see that the correlation between \"\n",
    "    \"x and y is ...\",\n",
    "    \"stderr\": None,\n",
    "    \"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"},\n",
    "}\n",
    "\n",
    "ToolMessage(\n",
    "    content=tool_output[\"stdout\"],\n",
    "    artifact=tool_output,\n",
    "    tool_call_id=\"call_Jja7J89XsjrOLA5r!MEOW!SL\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e7e19f",
   "metadata": {},
   "source": [
    "##### 5. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc04f652",
   "metadata": {},
   "source": [
    "##### 6. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed2b2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import init_embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2055ba31",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:68\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sentence_transformers'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# init_embeddings(\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#     model: str, *, provider: str | None = None, **kwargs: Any\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ) \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model_emb = \u001b[43minit_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-embedding-001\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuggingface\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m model_emb.embed_query(\u001b[33m\"\u001b[39m\u001b[33mHello, world!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langchain\\embeddings\\base.py:223\u001b[39m, in \u001b[36minit_embeddings\u001b[39m\u001b[34m(model, provider, **kwargs)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider == \u001b[33m\"\u001b[39m\u001b[33mhuggingface\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider == \u001b[33m\"\u001b[39m\u001b[33mollama\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_ollama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OllamaEmbeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phamx\\OneDrive\\Documents\\Github\\langchain-learn\\env\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:74\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     70\u001b[39m     msg = (\n\u001b[32m     71\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import sentence_transformers python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mbackend\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mipex\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_optimum_intel_available() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_ipex_available():\n",
      "\u001b[31mImportError\u001b[39m: Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`."
     ]
    }
   ],
   "source": [
    "# init_embeddings(\n",
    "#     model: str, *, provider: str | None = None, **kwargs: Any\n",
    "# ) \n",
    "model_emb = init_embeddings(\"gemini-embedding-001\", provider=\"huggingface\")\n",
    "model_emb.embed_query(\"Hello, world!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae561058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
